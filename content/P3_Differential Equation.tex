\chapter{Ordinary Differential Equation}

\section{Typology of Differential Equation}

\section{Solution(s) of ODE}

\subsection{Solution Space of ODE}
\begin{theorem}[Solution Space of Homogeneous Linear Differential Equations]
Consider the n-th order linear homogeneous differential equation:
\[
a_n(x)\frac{d^ny}{dx^n} + a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}} + \cdots + a_1(x)\frac{dy}{dx} + a_0(x)y = 0
\]
where $a_i(x)$ are continuous functions on an interval $I$ and $a_n(x) \neq 0$ for all $x \in I$. 
The set $S$ of all solutions to this equation forms a vector space over the field of real numbers.
\end{theorem}

\begin{proof}
To prove that $S$ is a vector space, we need to verify the eight vector space axioms:

1. Closure under addition: Let $y_1(x)$ and $y_2(x)$ be solutions to the equation. Then:
   \[
   a_n(x)\frac{d^n(y_1+y_2)}{dx^n} + \cdots + a_0(x)(y_1+y_2) = 
   \]
   \[
   \left(a_n(x)\frac{d^ny_1}{dx^n} + \cdots + a_0(x)y_1\right) + 
   \left(a_n(x)\frac{d^ny_2}{dx^n} + \cdots + a_0(x)y_2\right) = 0 + 0 = 0
   \]
   Thus, $y_1 + y_2$ is also a solution and belongs to $S$.

2. Closure under scalar multiplication: Let $y(x)$ be a solution to the equation and $c$ be any scalar. Then:
   \[
   a_n(x)\frac{d^n(cy)}{dx^n} + \cdots + a_0(x)(cy) = 
   c\left(a_n(x)\frac{d^ny}{dx^n} + \cdots + a_0(x)y\right) = c \cdot 0 = 0
   \]
   Thus, $cy$ is also a solution and belongs to $S$.

3. Commutativity of addition: For any $y_1, y_2 \in S$, clearly $y_1 + y_2 = y_2 + y_1$.

4. Associativity of addition: For any $y_1, y_2, y_3 \in S$, clearly $(y_1 + y_2) + y_3 = y_1 + (y_2 + y_3)$.

5. Additive identity: The zero function $y(x) = 0$ is a solution to the equation and acts as the additive identity.

6. Additive inverse: If $y(x)$ is a solution, then $-y(x)$ is also a solution, and $y + (-y) = 0$.

7. Scalar multiplication properties: For any scalars $c$ and $d$, and any $y, y_1, y_2 \in S$:
   \begin{itemize}
   \item $c(dy) = (cd)y$
   \item $1y = y$
   \item $(c+d)y = cy + dy$
   \item $c(y_1 + y_2) = cy_1 + cy_2$
   \end{itemize}
   These properties follow directly from the properties of real numbers and functions.

Therefore, the set $S$ of all solutions to the given linear homogeneous differential equation satisfies all the axioms of a vector space over the field of real numbers.
\end{proof}

\begin{theorem}[Solution Space of Non-Homogeneous Linear Differential Equations]
Consider the n-th order linear non-homogeneous differential equation:
\[
a_n(x)\frac{d^ny}{dx^n} + a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}} + \cdots + a_1(x)\frac{dy}{dx} + a_0(x)y = f(x)
\]
where $a_i(x)$ and $f(x)$ are continuous functions on an interval $I$ and $a_n(x) \neq 0$ for all $x \in I$.
The set $S$ of all solutions to this equation forms an affine space over the vector space $V$ of solutions to the corresponding homogeneous equation.
\end{theorem}
\begin{proof}
To prove that $S$ is an affine space, we need to verify the following properties:

Existence of a vector space action: Let $V$ be the vector space of solutions to the corresponding homogeneous equation. We define an action $+: S \times V \to S$ as follows:
For any $y \in S$ and $v \in V$, $y + v$ is defined as the usual addition of functions.
Associativity of the action: For any $y \in S$ and $v, w \in V$:
\[
(y + v) + w = y + (v + w)
\]
This follows from the associativity of function addition.

Identity element: The zero function $0 \in V$ acts as the identity element:
\[
y + 0 = y \quad \text{for all } y \in S
\]
Uniqueness of displacement vector: For any $y_1, y_2 \in S$, there exists a unique $v \in V$ such that $y_1 + v = y_2$.

To prove these properties:

Well-definedness of the action:
Let $y \in S$ and $v \in V$. Then:
\[
a_n(x)\frac{d^n(y+v)}{dx^n} + \cdots + a_0(x)(y+v) =
\]
\[
\left(a_n(x)\frac{d^ny}{dx^n} + \cdots + a_0(x)y\right) +
\left(a_n(x)\frac{d^nv}{dx^n} + \cdots + a_0(x)v\right) = f(x) + 0 = f(x)
\]
Thus, $y + v \in S$.
Associativity of the action: This follows directly from the associativity of function addition.
Identity element: The zero function clearly satisfies this property.
Uniqueness of displacement vector:
Let $y_1, y_2 \in S$. Define $v = y_2 - y_1$. Then:
\[
a_n(x)\frac{d^nv}{dx^n} + \cdots + a_0(x)v =
\]
\[
\left(a_n(x)\frac{d^ny_2}{dx^n} + \cdots + a_0(x)y_2\right) -
\left(a_n(x)\frac{d^ny_1}{dx^n} + \cdots + a_0(x)y_1\right) = f(x) - f(x) = 0
\]
Thus, $v \in V$, and clearly $y_1 + v = y_2$. Uniqueness follows from the fact that if $w \in V$ also satisfies $y_1 + w = y_2$, then $v - w = 0$, so $v = w$.
Affine combination property:
For $y_1, y_2, \ldots, y_n \in S$ and scalars $\alpha_1, \alpha_2, \ldots, \alpha_n$ with $\sum_{i=1}^n \alpha_i = 1$:
\[
\sum_{i=1}^n \alpha_i y_i = y_1 + \sum_{i=2}^n \alpha_i (y_i - y_1)
\]
Since each $y_i - y_1 \in V$ and $V$ is a vector space, $\sum_{i=2}^n \alpha_i (y_i - y_1) \in V$.
Therefore, $\sum_{i=1}^n \alpha_i y_i \in S$.

Therefore, the set $S$ of all solutions to the given linear non-homogeneous differential equation satisfies all the properties of an affine space over the vector space $V$ of solutions to the corresponding homogeneous equation.
\end{proof}

\subsection{Solvable Conditions of ODEs}

In most cases, it is sure that an n-order linear ODE is solvable (has at least one solution).

\begin{theorem}[Existence of Solutions for Linear ODE]
Consider an \(n\)-th order linear differential equation:
\[
L(y) = a_n(x) \frac{d^n y}{dx^n} + a_{n-1}(x) \frac{d^{n-1} y}{dx^{n-1}} + \cdots + a_1(x) \frac{dy}{dx} + a_0(x) y = f(x),
\]
where \(f(x)\) is a given function and \(a_0(x), a_1(x), \dots, a_n(x)\) are the coefficient functions.

If the right-hand side \(f(x)\) is a continuous function in some interval \([a,b]\), and the coefficient functions \(a_0(x), a_1(x), \dots, a_n(x)\) are also continuous on this interval, then a solution to the differential equation \textit{exists} on this interval.

This implies that regardless of the specific form of \(f(x)\), as long as it is continuous (even only continuous in [$a,b$]), and the coefficients are continuous as well, we can guarantee the existence of a solution in a certain interval.
\end{theorem}
In general, for ordinary differential equations, \textit{continuity} is more fundamental than \textit{differentiability}. If \(f(x)\) and the coefficient functions are continuous, then a solution exists. If \(f(x)\) and the coefficients are differentiable, then the solution may be smoother (i.e., higher-order derivatives exist), but the existence of the solution only requires continuity.

We can show this with Existence-Uniqueness Theorem when discuss the existence of solutions to linear ODEs later.

Now we discuss the existence and uniqueness of the particular solution to linear ODEs. First, it is pretty clear that, without extra \textbf{valid} information provided as in IVP or BVP, we can only get a general solution that contains a constant term $C$, which can be taken as a subset of $\mathbf{C}^n(\R)$: the set of all real functions that are n-th order differentiable.

When it comes to the existence of a distinct, particular solution, we may refer to Existence and Uniqueness Theorem.

\begin{theorem}[Existence-Uniqueness Theorem (Picard-Lindel√∂f)]
Let $\frac{dy}{dx} = f(x, y)$
be an ODE subject to the initial condition \( y(x_0) = y_0 \).

If:

(a) \( f(x, y) \) is continuous for every \( (x, y) \in D \), where \( D \) is a rectangle bounded by the straight lines \( x = x_0 \pm a \) and \( y = y_0 \pm b \) in \( \mathbb{R} \).

(b) \( f(x, y) \) satisfies the Lipschitz condition of order 1, namely:
\[
|f(x, y_1) - f(x, y_2)| \leq K |y_1 - y_2|,
\]
where \( K \) is a constant dependent on \( D \), then there exists a unique solution \( y = \bar{y}(x) \) to the ODE such that \( y_0 = \bar{y}(x_0) \) for all \( x \in [x_0 - \delta, x_0 + \delta] \), where
\[
\delta < \min \left\{ a, \frac{b}{M}, \frac{1}{K} \right\} \quad \text{and} \quad M = \max_{(x, y) \in D} f(x, y).
\]
\end{theorem}
\begin{remark}
    The proof the the theorem is not compulsory to most people, and may take some time to show the technique applied (Picard iteration), but you may find it \href{https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem}{here} in case that you already have some basics of real analysis.
\end{remark}
     We also show how can we derive Lipschitz Continuity from the general definition to continuity in function. For consistency with the theorem, we will give the definition and discuss in binary functions.
\begin{definition}[Continuity]
    A function \( f(x, y) \) is continuous at \( (x_0, y_0) \) if for any \( \epsilon > 0 \), there exists \( \delta > 0 \) such that:
\[
|(x, y) - (x_0, y_0)| < \delta \quad \implies \quad |f(x, y) - f(x_0, y_0)| < \epsilon.
\]
This ensures small changes in \( (x, y) \) produce small changes in \( f(x, y) \).
\end{definition}
Lipschitz continuity is a stricter condition. A function is Lipschitz continuous in \( y \) if there exists \( K > 0 \) such that for any \( x \in D \) and all \( y_1, y_2 \in D \),
\[
|f(x, y_1) - f(x, y_2)| \leq K |y_1 - y_2|.
\]
While continuity guarantees no jumps, Lipschitz continuity controls the rate of change, bounding it by \( K \).

Lipschitz continuity implies continuity: given \( f(x, y) \) Lipschitz continuous, for any \( y_1, y_2 \),
\[
|f(x, y_1) - f(x, y_2)| \leq K |y_1 - y_2|.
\]
By setting \( \delta = \frac{\epsilon}{K} \), if \( |y_1 - y_2| < \delta \), then \( |f(x, y_1) - f(x, y_2)| < \epsilon \), which satisfies the continuity condition. Thus, every Lipschitz continuous function is continuous, but not every continuous function is Lipschitz, as general continuity lacks a bound on the rate of change.

There are more worth discussing on Lipschitz condition, since in real practice, we tend to use it's equivalent condition, which is easier to prove. 
\begin{corollary}
    For functions that are differentiable with respect to \( y \), the Lipschitz condition can be equivalently expressed in terms of the partial derivative. If the partial derivative of \( f(x, y) \) with respect to \( y \) is bounded, i.e.,
\[
\left| \frac{\partial f(x, y)}{\partial y} \right| \leq K \quad \text{for all } (x, y) \in D,
\]
then the function is Lipschitz continuous with respect to \( y \). In this case, the Lipschitz constant \( K \) is exactly the upper bound on the partial derivative.
\end{corollary}
\begin{proof}
To prove that bounded partial derivatives imply Lipschitz continuity, we proceed as follows:

\begin{enumerate}
    \item Assume that \( f(x, y) \) is continuously differentiable with respect to \( y \). Then for any two points \( y_1 \) and \( y_2 \), the fundamental theorem of calculus gives:
\[
f(x, y_1) - f(x, y_2) = \int_{y_2}^{y_1} \frac{\partial f(x, t)}{\partial y} \, dt
\]
    \item Taking the absolute value and applying the boundedness condition \( \left| \frac{\partial f(x, y)}{\partial y} \right| \leq K \), we have:
\[
|f(x, y_1) - f(x, y_2)| \leq \int_{y_2}^{y_1} \left| \frac{\partial f(x, t)}{\partial y} \right| \, dt \leq K |y_1 - y_2|
\]
\end{enumerate}

This confirms that if the partial derivative is bounded, the function satisfies the Lipschitz condition.
\end{proof}
 Thus, Lipschitz continuity can be viewed as an extension of the concept of bounded derivatives, guaranteeing controlled changes in the function's output as \( y \) varies.



\section{Separable ODEs}

\section{First-order linear homogeneous ODEs}

\section{First-order linear non-homogeneous ODEs}

\begin{theorem}[Solution of First-Order Linear Non-Homogeneous ODE]
Consider the first-order linear non-homogeneous ordinary differential equation:
\[
\frac{dy}{dx} + p(x)y = q(x)
\]
where $p(x)$ and $q(x)$ are continuous functions on an interval $I$. The general solution to this equation is given by:
\[
y(x) = \frac{1}{I(x)}\left[\int I(x)q(x)dx + C\right]
\]
where $I(x) = e^{\int p(x)dx}$ is an integrating factor, and $C$ is an arbitrary constant.
\end{theorem}

\begin{proof}
To solve this differential equation, we employ the method of integrating factors. Our goal is to find a function $I(x)$ such that when we multiply both sides of the equation by $I(x)$, the left-hand side becomes the derivative of a product.

We begin by multiplying both sides of the equation by an as-yet-undetermined function $I(x)$:

\[
I(x)\frac{dy}{dx} + I(x)p(x)y = I(x)q(x)
\]

Now, we aim to choose $I(x)$ such that the left-hand side is the derivative of the product $I(x)y$. Expanding this derivative using the product rule, we get:

\[
\frac{d}{dx}(I(x)y) = I(x)\frac{dy}{dx} + y\frac{dI}{dx}
\]

Comparing this with the left-hand side of our equation, we see that we need:

\[
y\frac{dI}{dx} = I(x)p(x)y
\]

This equality should hold for all $y$, so we can conclude:

\[
\frac{dI}{dx} = I(x)p(x)
\]

This is a separable differential equation for $I(x)$. Solving it, we find:

\[
I(x) = e^{\int p(x)dx}
\]

This is our integrating factor. With this choice of $I(x)$, our original equation becomes:

\[
\frac{d}{dx}(I(x)y) = I(x)q(x)
\]

We can now integrate both sides:

\[
I(x)y = \int I(x)q(x)dx + C
\]

where $C$ is an arbitrary constant of integration. Finally, solving for $y$, we obtain our general solution:

\[
y(x) = \frac{1}{I(x)}\left[\int I(x)q(x)dx + C\right]
\]

This completes our proof.
\end{proof}

\section{Second-order linear homogeneous ODEs}
\begin{theorem}[Solution of Second-Order Linear Homogeneous ODE]
Consider the second-order linear homogeneous ordinary differential equation:
\[
\frac{d^2 y}{dx^2} + p(x)\frac{dy}{dx} + q(x)y = 0
\]
where $p(x)$ and $q(x)$ are continuous functions on an interval $I$. The general solution to this equation is given by a linear combination of two independent solutions $y_1(x)$ and $y_2(x)$:
\[
y(x) = C_1 y_1(x) + C_2 y_2(x)
\]
where $C_1$ and $C_2$ are arbitrary constants.
\end{theorem}

\begin{proof}
The general solution to a second-order linear homogeneous ODE can be found by solving the characteristic equation associated with the differential equation.

For constant coefficients, the equation is of the form:
\[
\frac{d^2 y}{dx^2} + a \frac{dy}{dx} + b y = 0
\]
where $a$ and $b$ are constants. To solve this, we assume the solution is of the form $y = e^{rx}$, where $r$ is a constant. Substituting this into the differential equation gives the characteristic equation:
\[
r^2 + a r + b = 0
\]

Solving this quadratic equation, we obtain three cases for the roots $r_1$ and $r_2$:

\begin{itemize}
    \item If $r_1 \neq r_2$ (distinct real roots), the general solution is:
    \[
    y(x) = C_1 e^{r_1 x} + C_2 e^{r_2 x}
    \]
    \item If $r_1 = r_2$ (repeated root), the general solution is:
    \[
    y(x) = (C_1 + C_2 x) e^{r_1 x}
    \]
    \item If $r_1$ and $r_2$ are complex conjugates ($r_1 = \alpha + i\beta$, $r_2 = \alpha - i\beta$), the general solution is:
    \[
    y(x) = e^{\alpha x}(C_1 \cos(\beta x) + C_2 \sin(\beta x))
    \]
\end{itemize}

In the case of non-constant coefficients, finding solutions involves more advanced techniques such as reduction of order or using known solutions, but the general solution remains a linear combination of two independent solutions. This completes our proof.
\end{proof}
\section{Second-order linear non-homogeneous ODEs}
\begin{theorem}[Solution of Second-Order Linear Non-Homogeneous ODE]
Consider the second-order linear non-homogeneous ordinary differential equation:
\[
\frac{d^2 y}{dx^2} + p(x)\frac{dy}{dx} + q(x)y = g(x)
\]
where $p(x)$, $q(x)$, and $g(x)$ are continuous functions on an interval $I$. The general solution to this equation is the sum of the general solution of the corresponding homogeneous equation and a particular solution to the non-homogeneous equation:
\[
y(x) = y_h(x) + y_p(x)
\]
where $y_h(x)$ is the general solution to the homogeneous equation and $y_p(x)$ is a particular solution to the non-homogeneous equation.
\end{theorem}

\begin{proof}
The solution to a second-order linear non-homogeneous ODE involves two steps:

1. Solve the corresponding homogeneous equation:
\[
\frac{d^2 y}{dx^2} + p(x)\frac{dy}{dx} + q(x)y = 0
\]
The general solution to this equation is denoted as $y_h(x)$, which is found using the method described in the previous section.

2. Find a particular solution to the non-homogeneous equation:
\[
\frac{d^2 y}{dx^2} + p(x)\frac{dy}{dx} + q(x)y = g(x)
\]
The method used to find $y_p(x)$ depends on the form of $g(x)$. Common methods include:
\begin{itemize}
    \item Method of undetermined coefficients: This method works when $g(x)$ is a polynomial, exponential, or a sine/cosine function. A trial solution is proposed, and the coefficients are determined by substituting into the differential equation.
    \item Variation of parameters: This method is more general and can be applied when the method of undetermined coefficients is not suitable. It uses the solutions of the homogeneous equation to construct a particular solution.
\end{itemize}

The complete solution is then the sum of the homogeneous solution and the particular solution:
\[
y(x) = y_h(x) + y_p(x)
\]

This completes our proof.
\end{proof}
\subsubsection*{Method of Undetermined Coefficients: Techniques Guessing the Particular Solution}
In the method of undetermined coefficients, we guess the form of the particular solution \( y_p(x) \) based on the form of the non-homogeneous term \( r(x) \). Below is a table summarizing common forms of \( r(x) \) and their corresponding guesses for \( y_p(x) \). If the guessed particular solution overlaps with the homogeneous solution, we multiply the guess by a power of \( x \) to ensure linear independence.

\begin{table}[h!]
\centering
\begin{tabular}{|p{6cm}|p{8cm}|}
\hline
\textbf{Non-homogeneous term \( r(x) \)} & \textbf{Guess for particular solution \( y_p(x) \)} \\
\hline
\( C \) & \( A \) \\
\hline
\( ax + b \) & \( A x + B \) \\
\hline
\( ax^2 + bx + c \) & \( A x^2 + B x + C \) \\
\hline
\( e^{\alpha x} \) & \( A e^{\alpha x} \) \\
\hline
\( x^n e^{\alpha x} \) & \( (A_n x^n + A_{n-1} x^{n-1} + \dots + A_0) e^{\alpha x} \) \\
\hline
\( \sin(\beta x) \text{ or } \cos(\beta x) \) & \( A \sin(\beta x) + B \cos(\beta x) \) \\
\hline
\( x^n \sin(\beta x) \text{ or } x^n \cos(\beta x) \) & \( (A_n x^n + A_{n-1} x^{n-1} + \dots + A_0) \sin(\beta x) + (B_n x^n + B_{n-1} x^{n-1} + \dots + B_0) \cos(\beta x) \) \\
\hline
\( e^{\alpha x} \sin(\beta x) \text{ or } e^{\alpha x} \cos(\beta x) \) & \( e^{\alpha x} (A \cos(\beta x) + B \sin(\beta x)) \) \\
\hline
\end{tabular}
\caption{Common non-homogeneous terms and corresponding guesses for particular solutions.}
\end{table}
The table provides common forms of non-homogeneous terms \( r(x) \) and the corresponding guesses for the particular solution \( y_p(x) \). Here's a breakdown of the logic behind the guesses:

\begin{itemize}
    \item \textbf{Constant terms}: For constant non-homogeneous terms, we guess that the particular solution is also a constant, \( y_p(x) = A \). If the constant already appears in the homogeneous solution, we multiply by \( x \) to ensure linear independence.
    
    \item \textbf{Polynomials}: For polynomial non-homogeneous terms, we guess that the particular solution is a polynomial of the same degree. For example, if \( r(x) = ax^2 + bx + c \), then \( y_p(x) \) is guessed to be \( A x^2 + B x + C \). If part of this polynomial appears in the homogeneous solution, we multiply by \( x \) to create a new linearly independent solution.
    
    \item \textbf{Exponential terms}: For an exponential non-homogeneous term \( r(x) = e^{\alpha x} \), we guess that the particular solution is also of the form \( y_p(x) = A e^{\alpha x} \). If \( e^{\alpha x} \) is part of the homogeneous solution, we multiply by \( x \) (or higher powers of \( x \)) to maintain linear independence.
    
    \item \textbf{Sine and Cosine terms}: When \( r(x) \) involves trigonometric functions like \( \sin(\beta x) \) or \( \cos(\beta x) \), we guess that the particular solution is a linear combination of both sine and cosine, i.e., \( y_p(x) = A \sin(\beta x) + B \cos(\beta x) \). If \( \sin(\beta x) \) or \( \cos(\beta x) \) already appears in the homogeneous solution, we multiply by \( x \).
    
    \item \textbf{Products of polynomials and exponentials or trigonometric functions}: When the non-homogeneous term is a product of a polynomial and an exponential or trigonometric function, the particular solution is guessed as a polynomial multiplied by the exponential or trigonometric term. For example, if \( r(x) = x^n e^{\alpha x} \), the guess is \( y_p(x) = (A_n x^n + A_{n-1} x^{n-1} + \dots + A_0) e^{\alpha x} \). If there's overlap with the homogeneous solution, multiply by \( x \).
\end{itemize}

\noindent The key principle behind these guesses is that the particular solution must match the form of the non-homogeneous term \( r(x) \), while maintaining linear independence from the homogeneous solution. If any part of the guessed particular solution overlaps with the homogeneous solution, multiplying by \( x \) (or higher powers of \( x \)) ensures that the solution is linearly independent.